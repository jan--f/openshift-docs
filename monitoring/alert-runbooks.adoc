// This file and the runbooks were generated by https://github.com/openshift/runbooks/tree/beaf8113f6c692ea509fc7893bb3bbbef8f26501/hack/convert-runbooks-to-doc.sh
[id="alert-runbooks"]
= Alert Runbooks
include::modules/attributes-openshift-dedicated.adoc[]

toc::[]

[id="aggregatedapierrors"]
== AggregatedAPIErrors

[discrete]
=== Meaning

https://github.com/openshift/cluster-monitoring-operator/blob/aefc8fc5fc61c943dc1ca24b8c151940ae5f8f1c/assets/control-plane/prometheus-rule.yaml#L440-L449[This alert] is triggered when multiple calls to the
aggregated API of OpenShift fail over a certain period.

[discrete]
=== Impact

Errors on the aggregated API can result in the unavailability of some OpenShift
services.

[discrete]
=== Diagnosis

The alert should contain information about the affected API and the scope of the
impact.

[,text]
----
 - alertname = AggregatedAPIErrors
 - name = v1.packages.operators.coreos.com
 - namespace = default
...
 - message = An aggregated API v1.packages.operators.coreos.com/default has reported errors. The number of errors have increased for it in the past five minutes. High values indicate that the availability of the service changes too often.
----

[discrete]
=== Mitigation

[discrete]
==== Check the APIs status checks are on True

Currently, there are at least four aggregated APIs in an OpenShift Cluster. The
API on the `openshift-apiserver` namespace, the prometheus-adapter on the
namespace `openshift-monitoring`, the package-server service in the
`openshift-operator-lifecycle-manager` namespace, and the API on the
`openshift-oauth-apiserver` namespace. However, it makes sense to check the
availability of all APIs.

To get a list of `APIServices` and their backing aggregated APIs, use the
following command:

[source,terminal]
----
$ oc get apiservice
----

The `SERVICE` column notes here the aggregated API name. The availability status
for every listed API should be `True`. A `False` means that requests for that
API service, API server pods, or resources belonging to that apiGroup failed
many times during the last minutes.

Fetch the pods that serve the unavailable API. E.g.: for
`openshift-apiserver/api` use the following command:

[source,terminal]
----
$ oc get pods -n openshift-apiserver
----

When their status is not `Running`, check the logs for more details. As these
pods are controlled by a deployment, they can be restart when they are not
answering to requests anymore.

[discrete]
==== Check the authentication certificates of the aggregated API

Make sure the certificates are up to date and still valid. Use:

[source,terminal]
----
$ oc get configmaps -n kube-system extension-apiserver-authentication
----

You can save those certificates into a file and use the following command to
check the end dates:

[source,terminal]
----
$ openssl x509 -noout -enddate -in {myfile_with_certs.crt}
----

Those certificates are used by the aggregated APIs to validate requests. For the
case, they are expired check https://docs.openshift.com/container-platform/latest/security/certificates/api-server.html[here] how to add a new one.

[id="highlyavailableworkloadincorrectlyspread"]
== HighlyAvailableWorkloadIncorrectlySpread

[discrete]
=== Meaning

Highly available workloads with persistent storage are incorrectly spread across
multiple nodes. In high availability topology, all workloads with multiple
replicas should be spread across multiple nodes to avoid having a single point
of failure. However, workloads with persistent storage are more complex to
schedule since persistent volumes may be bound to an availability zone, so they
need to be spread manually before any scheduling constraints are put in place.

[discrete]
=== Impact

The cluster isn't fully highly available since some workloads with persistent
storage still have a single point of failure.

[discrete]
=== Diagnosis

The alert indicates which workloads aren't correctly spread across multiple
nodes. You can find information to identify them under the `workload` and
`namespace` labels.

[source,terminal]
----
 - alertname = HighlyAvailableWorkloadIncorrectlySpread
...
 - workload = prometheus-k8s
...
 - namespace = openshift-monitoring
----

Then you can verify that multiple instances of the same application are indeed
scheduled on the same node by running:

[source,terminal]
----
$ oc -n "$NS" get -o wide pods | grep "$WORKLOAD"
----

[discrete]
=== Mitigation

To mitigate this issue, you can follow the steps below, but first you should
get the following information from the alert labels:

* `workload`: workload name
* `namespace`: namespace of the workload

First, you need to get the name of the node hosting all the instances of your
workload:

[source,terminal]
----
$ NODE=$(oc -n "$NS" get pods -ojson | jq -r --arg WORKLOAD "$WORKLOAD" '.items[] | select(.metadata.name|test($WORKLOAD + ".*")) | .spec.nodeName' | head -n 1)
----

Now that you have the name of the node, you will have to cordon it to prevent
pods from being rescheduled on this node:

[source,terminal]
----
$ oc adm cordon "$NODE"
----

Then you will need to get the list of pods that should be rescheduled for the
given workload and node:

[source,terminal]
----
$ POD=$(oc -n "$NS" get -o wide pods | grep "$WORKLOAD.*$NODE" | cut -f1 -d ' ' | head -n 1)
----

If your storage system is bound to availability zones, you will also want to
get the number of availability zones present on your cluster to know if you
need to delete persistent volume claims:

[source,terminal]
----
$ oc get nodes -o yaml | grep -E "^\s+failure-domain.beta.kubernetes.io/zone" | uniq | wc -l
----

Then for each pod, you will want to delete the pod and the PVC attached to it,
if required by the previous step, in order to reschedule it on another node:

____
Note that deleting the PVC will result in data loss and the replication level
will decrease, but this is a required step if you want your cluster to be
highly available.
____

[source,terminal]
----
$ PVC=$(oc get -n "$NS" pod "$POD" -ojson | jq -r '.spec.volumes[] | select(.persistentVolumeClaim!=null) | .persistentVolumeClaim.claimName')
$ oc delete -n "$NS" pvc "$PVC"
$ oc delete -n "$NS" pod "$POD"
----

Once all the pods are rescheduled, you can uncordon the node:

[source,terminal]
----
$ oc adm uncordon "$NODE"
----

[id="etcdbackendquotalowspace"]
== etcdBackendQuotaLowSpace

[discrete]
=== Meaning

This alert fires when the total existing DB size exceeds 95% of the maximum
DB quota. The consumed space is in Prometheus represented by the metric
`etcd_mvcc_db_total_size_in_bytes`, and the DB quota size is defined by
`etcd_server_quota_backend_bytes`.

[discrete]
=== Impact

In case the DB size exceeds the DB quota, no writes can be performed anymore on
the etcd cluster. This further prevents any updates in the cluster, such as the
creation of pods.

[discrete]
=== Diagnosis

The following two approaches can be used for the diagnosis.

[discrete]
==== CLI Checks

To run `etcdctl` commands, we need to `rsh` into the `etcdctl` container of any
etcd pod.

[source,terminal]
----
$ oc rsh -c etcdctl -n openshift-etcd $(oc get po -l app=etcd -oname -n openshift-etcd | awk -F"/" 'NR==1{ print $2 }')
----

Validate that the `etcdctl` command is available:

[source,terminal]
----
$ etcdctl version
----

`etcdctl` can be used to fetch the DB size of the etcd endpoints.

[source,terminal]
----
$ etcdctl endpoint status -w table
----

[discrete]
==== PromQL queries

Check the percentage consumption of etcd DB with the following query in the
metrics console:

[source,terminal]
----
(etcd_mvcc_db_total_size_in_bytes / etcd_server_quota_backend_bytes) * 100
----

Check the DB size in MB that can be reduced after defragmentation:

[source,terminal]
----
(etcd_mvcc_db_total_size_in_bytes - etcd_mvcc_db_total_size_in_use_in_bytes)/1024/1024
----

[discrete]
=== Mitigation

[discrete]
==== Capacity planning

If the `etcd_mvcc_db_total_size_in_bytes` shows that you are growing close to
the `etcd_server_quota_backend_bytes`, etcd almost reached max capacity and it's
start planning for new cluster.

In the meantime before migration happens, you can use defrag to gain some time.

[discrete]
==== Defrag

When the etcd DB size increases, we can defragment existing etcd DB to optimize
DB consumption as described in https://etcd.io/docs/v3.4.0/op-guide/maintenance/[here]. Run the following
command in all etcd pods.

[source,terminal]
----
$ etcdctl defrag
----

As validation, check the endpoint status of etcd members to know the reduced
size of etcd DB. Use for this purpose the same diagnostic approaches as listed
above. More space should be available now.

[id="etcdgrpcrequestsslow"]
== etcdGRPCRequestsSlow

[discrete]
=== Meaning

This alert fires when the 99th percentile of etcd gRPC requests are too slow.

[discrete]
=== Impact

When requests are too slow, they can lead to various scenarios like leader
election failure, slow reads and writes.

[discrete]
=== Diagnosis

This could be result of slow disk (due to fragmented state) or CPU contention.

[discrete]
==== Slow disk

One of the most common reasons for slow gRPC requests is disk. Checking disk
related metrics and dashboards should provide a more clear picture.

[discrete]
===== PromQL queries used to troubleshoot

Verify the value of how slow the etcd gRPC requests are by using the following
query in the metrics console:

[source,terminal]
----
histogram_quantile(0.99, sum(rate(grpc_server_handling_seconds_bucket{job=~".*etcd.*", grpc_type="unary"}[5m])) without(grpc_type))
----

That result should give a rough timeline of when the issue started.

`etcd_disk_wal_fsync_duration_seconds_bucket` reports the etcd disk fsync
duration, `etcd_server_leader_changes_seen_total` reports the leader changes. To
rule out a slow disk and confirm that the disk is reasonably fast, 99th
percentile of the etcd_disk_wal_fsync_duration_seconds_bucket should be less
than 10ms. Query in metrics UI:

[source,terminal]
----
histogram_quantile(0.99, sum by (instance, le) (irate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])))
----

[discrete]
===== Console dashboards

In the OpenShift dashboard console under Observe section, select the etcd
dashboard. There are both RPC rate as well as Disk Sync Duration dashboards
which will assist with further issues.

[discrete]
==== Resource exhaustion

It can happen that etcd responds slower due to CPU resource exhaustion.
This was seen in some cases when one application was requesting too much CPU
which led to this alert firing for multiple methods.

Often if this is the case, we also see
`etcd_disk_wal_fsync_duration_seconds_bucket` slower as well.

To confirm this is the cause of the slow requests either:

. In OpenShift console on primary page under "Cluster utilization" view the
requested CPU vs available.
. PromQL query is the following to see top consumers of CPU:

[source,terminal]
----
      topk(25, sort_desc(
        sum by (namespace) (
          (
            sum(avg_over_time(pod:container_cpu_usage:sum{container="",pod!=""}[5m])) BY (namespace, pod)
            *
            on(pod,namespace) group_left(node) (node_namespace_pod:kube_pod_info:)
          )
          *
          on(node) group_left(role) (max by (node) (kube_node_role{role=~".+"}))
        )
      ))
----

[discrete]
=== Mitigation

[discrete]
==== Fragmented state

In the case of slow fisk or when the etcd DB size increases, we can defragment
existing etcd DB to optimize DB consumption as described in
https://etcd.io/docs/v3.4.0/op-guide/maintenance/[here]. Run the following command in all etcd pods.

[source,terminal]
----
$ etcdctl defrag
----

As validation, check the endpoint status of etcd members to know the reduced
size of etcd DB. Use for this purpose the same diagnostic approaches as listed
above. More space should be available now.

Further info on etcd best practices can be found in the https://docs.openshift.com/container-platform/4.7/scalability_and_performance/recommended-host-practices.html#recommended-etcd-practices_[OpenShift docs
here].

[id="etcdhighfsyncdurations"]
== etcdHighFsyncDurations

[discrete]
=== Meaning

This alert fires when the 99th percentile of etcd disk fsync duration is too
high for 10 minutes.

[discrete]
=== Impact

When this happens it can lead to various scenarios like leader election failure,
frequent leader elections, slow reads and writes.

[discrete]
=== Diagnosis

This could be result of slow disk possibly due to fragmented state in etcd or
simply due to slow disk.

[discrete]
==== Slow disk

Checking disk related metrics and dashboards should provide a more clear
picture.

[discrete]
===== PromQL queries used to troubleshoot

`etcd_disk_wal_fsync_duration_seconds_bucket` reports the etcd disk fsync
duration, `etcd_server_leader_changes_seen_total` reports the leader changes. To
rule out a slow disk and confirm that the disk is reasonably fast, 99th
percentile of the etcd_disk_wal_fsync_duration_seconds_bucket should be less
than 10ms. Query in metrics UI:

[source,terminal]
----
histogram_quantile(0.99, sum by (instance, le) (irate(etcd_disk_wal_fsync_duration_seconds_bucket{job="etcd"}[5m])))
----

[discrete]
===== Console dashboards

In the OpenShift dashboard console under Observe section, select the etcd
dashboard. There are both leader elections as well as Disk Sync Duration
dashboards which will assit with further issues.

[discrete]
=== Mitigation

[discrete]
==== Fragmented state

In the case of slow fisk or when the etcd DB size increases, we can defragment
existing etcd DB to optimize DB consumption as described in
https://etcd.io/docs/v3.4.0/op-guide/maintenance/[here]. Run the following command in all etcd pods.

[source,terminal]
----
$ etcdctl defrag
----

As validation, check the endpoint status of etcd members to know the reduced
size of etcd DB. Use for this purpose the same diagnostic approaches as listed
above. More space should be available now.

Further info on etcd best practices can be found in the https://docs.openshift.com/container-platform/4.7/scalability_and_performance/recommended-host-practices.html#recommended-etcd-practices_[OpenShift docs
here].

[id="etcdhighnumberoffailedgrpcrequests"]
== etcdHighNumberOfFailedGRPCRequests

[discrete]
=== Meaning

This alert fires when at least 50% of etcd gRPC requests failed in the past 10
minutes.

[discrete]
=== Impact

First establish which gRPC method is failing, this will be visible in the alert.
If it's not part of the alert, the following query will display method and etcd
instance that has failing requests:

[,sh]
----
100 * sum without(grpc_type, grpc_code)
(rate(grpc_server_handled_total{grpc_code=~"Unknown|FailedPrecondition|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded",job="etcd"}[5m]))
/ sum without(grpc_type, grpc_code)
(rate(grpc_server_handled_total{job="etcd"}[5m])) > 5 and on()
(sum(cluster_infrastructure_provider{type!~"ipi|BareMetal"} == bool 1))
----

[discrete]
=== Diagnosis

All the gRPC errors should also be logged in each respective etcd instance logs.
You can get the instance name from the alert that is firing or by running the
query detailed above. Those etcd instance logs should serve as further insight
into what is wrong.

To get logs of etcd containers either check the instance from the alert and
check logs directly or run the following:

[,sh]
----
oc logs -n openshift-etcd -lapp=etcd etcd
----

[discrete]
==== Defrag method errors

If defrag method is failing, this could be due to defrag that is periodically
performed by cluster-etcd-operator pe starting from OpenShift v4.9 onwards. To
verify this check the logs of cluster-etcd-operator.

[,sh]
----
oc logs -l app=etcd-operator -n openshift-etcd-operator --tail=-1
----

If you have run defrag manually on older OpenShift versions check the errors of
those manual runs.

[discrete]
==== MemberList method errors

Member list is most likely performed by cluster-etcd-operator, so it's also best
to check also logs of cluster-etcd-operator for any errors:

[,sh]
----
oc logs -l app=etcd-operator -n openshift-etcd-operator --tail=-1
----

[discrete]
=== Mitigation

Depending on the above diagnosis, the issue will most likely be described in the
error log line of either etcd or openshift-etcd-operator. Most likely causes
tend to be networking issues.

[id="etcdinsufficientmembers"]
== etcdInsufficientMembers

[discrete]
=== Meaning

This alert fires when there are fewer instances available than are needed by
etcd to be healthy.

[discrete]
=== Impact

When etcd does not have a majority of instances available the Kubernetes and
OpenShift APIs will reject read and write requests and operations that preserve
the health of workloads cannot be performed.

[discrete]
=== Diagnosis

This can occur multiple control plane nodes are powered off or are unable to
connect each other via the network. Check that all control plane nodes are
powered and that network connections between each machine are functional.

Check any other critical, warning or info alerts firing that can assist with the
diagnosis.

Login to the cluster. Check health of master nodes if any of them is in
`NotReady` state or not.

[source,terminal]
----
$ oc get nodes -l node-role.kubernetes.io/master=
----

Check if an upgrade is in progress.

[source,terminal]
----
$ oc adm upgrade
----

In case there is no upgrade going on, but there is a change in the
`machineconfig` for the master pool causing a rolling reboot of each master
node, this alert can be triggered as well. We can check if the
`machineconfiguration.openshift.io/state : Working` annotation is set for any of
the master nodes. This is the case when the https://github.com/openshift/machine-config-operator[machine-config-operator
(MCO)] is working on it.

[source,terminal]
----
[discrete]
$ oc get nodes -l node-role.kubernetes.io/master= -o template --template='{{range .items}}{{"===> node:> "}}{{.metadata.name}}{{"\n"}}{{range $k, $v := .metadata.annotations}}{{println $k ":" $v}}{{end}}{{"\n"}}{{end}}'
----

[discrete]
==== General etcd health

To run `etcdctl` commands, we need to `rsh` into the `etcdctl` container of any
etcd pod.

[source,terminal]
----
$ oc rsh -c etcdctl -n openshift-etcd $(oc get po -l app=etcd -oname -n openshift-etcd | awk -F"/" 'NR==1{ print $2 }')
----

Validate that the `etcdctl` command is available:

[source,terminal]
----
$ etcdctl version
----

Run the following command to get the health of etcd:

[source,terminal]
----
$ etcdctl endpoint health -w table
----

[discrete]
=== Mitigation

[discrete]
==== Disaster and recovery

If an upgrade is in progress, the alert may automatically resolve in some time
when the master node comes up again. If MCO is not working on the master node,
check the cloud provider to verify if the master node instances are running or not.

In the case when you are running on AWS, the AWS instance retirement might need
a manual reboot of the master node.

As a last resort if none of the above fix the issue and the alert is still
firing, for etcd specific issues follow the steps described in the link:docs[disaster and
recovery docs].

[id="etcdmembersdown"]
== etcdMembersDown

[discrete]
=== Meaning

This alert fires when one or more etcd member goes down and evaluates the
number of etcd members that are currently down. Often, this alert was observed
as part of a cluster upgrade when a master node is being upgraded and requires a
reboot.

[discrete]
=== Impact

In etcd a majority of (n/2)+1 has to agree on membership changes or key-value
upgrade proposals. With this approach, a split-brain inconsistency can be
avoided. In the case that only one member is down in a 3-member cluster, it
still can make forward progress. Due to the fact that the quorum is 2 and 2
members are still alive. However, when more members are down, the cluster
becomes unrecoverable.

[discrete]
=== Diagnosis

Login to the cluster. Check health of master nodes if any of them is in
`NotReady` state or not.

[source,terminal]
----
$ oc get nodes -l node-role.kubernetes.io/master=
----

Check if an upgrade is in progress.

[source,terminal]
----
$ oc adm upgrade
----

In case there is no upgrade going on, but there is a change in the
`machineconfig` for the master pool causing a rolling reboot of each master
node, this alert can be triggered as well. We can check if the
`machineconfiguration.openshift.io/state : Working` annotation is set for any of
the master nodes. This is the case when the https://github.com/openshift/machine-config-operator[machine-config-operator
(MCO)] is working on it.

[source,terminal]
----
[discrete]
$ oc get nodes -l node-role.kubernetes.io/master= -o template --template='{{range .items}}{{"===> node:> "}}{{.metadata.name}}{{"\n"}}{{range $k, $v := .metadata.annotations}}{{println $k ":" $v}}{{end}}{{"\n"}}{{end}}'
----

[discrete]
==== General etcd health

To run `etcdctl` commands, we need to `rsh` into the `etcdctl` container of any
etcd pod.

[source,terminal]
----
$ oc rsh -c etcdctl -n openshift-etcd $(oc get po -l app=etcd -oname -n openshift-etcd | awk -F"/" 'NR==1{ print $2 }')
----

Validate that the `etcdctl` command is available:

[source,terminal]
----
$ etcdctl version
----

Run the following command to get the health of etcd:

[source,terminal]
----
$ etcdctl endpoint health -w table
----

[discrete]
=== Mitigation

If an upgrade is in progress, the alert may automatically resolve in some time
when the master node comes up again. If MCO is not working on the master node,
check the cloud provider to verify if the master node instances are running or not.

In the case when you are running on AWS, the AWS instance retirement might need
a manual reboot of the master node.

[id="etcdnoleader"]
== etcdNoLeader

[discrete]
=== Meaning

This alert is triggered when etcd cluster does not have a leader for more than 1
minute.

[discrete]
=== Impact

When there is no leader, Kubernetes and OpenShift APIs will not be able to work
as expected and cluster cannot process any writes or reads, and any write
requests are queued for processing until a new leader is elected. Operations
that preserve the health of the workloads cannot be performed.

[discrete]
=== Diagnosis

[discrete]
==== Control plane nodes issue

This can occur multiple control plane nodes are powered off or are unable to
connect each other via the network. Check that all control plane nodes are
powered and that network connections between each machine are functional.

[discrete]
==== Slow disk issue

Another potential cause could be slow disk, inspect the ``Disk Sync
Duration``dashboard, as well as the `Total Leader Elections Per Day` to get more
insight and help with diagnosis. Both dashboards are located in the OpenShift
console under `etcd` dashboard.

[discrete]
==== Other

Check the logs of etcd containers to see any further information and to verify
that etcd does not have leader. Logs should contain something like `etcdserver:
no leader`. etcd containers are running in the `openshift-etcd`
namespace/project and in the `etcd` container.

[discrete]
=== Mitigation

[discrete]
==== Disaster and recovery

Follow the steps described in the link:docs[disaster and recovery docs].

[id="auditlogerror"]
== AuditLogError

[discrete]
=== Meaning

This alert is triggered when an API Server instance in the cluster is unable
to write audit logs. It fires when there's any errors, which is calculated
by checking the error rate with `apiserver_audit_error_total` and `apiserver_audit_event_total`.

There might be many causes to this:

* This might have been caused by the node that host's that API Server instance
running out of disk space.
* A malicious actor could be tampering with the audit log files or directory
permissions
* The API server might be encountering an unexpected error.

[discrete]
=== Impact

When there are errors writing audit logs, security events will not be logged
by that specific API Server instance. Security Incident Response teams use
these audit logs, amongst other artifacts, to determine the impact of
security breaches or events. Without these logs, it becomes very difficult
to assess a situation and do appropriate root cause analysis in such incidents.

However, this is not detrimental to the cluster's availability.

[discrete]
=== Diagnosis

Verify if there are other alerts being triggered, e.g. the
`NodeFilesystemFillingUp` alert. This might indicate what's causing this error.

The metric `apiserver_audit_error_total` will only show up on instances
that are experiencing errors. There will be appropriate labels that indicate
the API Server type and the specific instance that's affected.

With this information, gather the runtime logs from that specific api server
pod, and verify if the logs indicate an unexpected error.

From the `instance` label, it's possible to determine what node is hosting
the aforementioned affected API Server.

Log into the appropriate node and Verify that the relevant audit log file
permissions are what's expected:
Owned by the `root` user and with a mode of `0600`. Make sure as well that
there aren't unexpected attributes for the files, such as immutability or append-only.

While logged into the appropriate node, also verify that the relevant audit
log directory permissions are what's expected:
Owned by the `root` user and with a mode of `0700`.

If you suspect tampering is happening, contact your incident response team.

[discrete]
=== Mitigation

The appropriate mitigation will be very different depending on the organization
and the compliance requirements. A FedRAMP moderate deployment might need to
isolate the node and investigate, while deployment with more strict compliance
requirements would need to snapshot and shut down the system immediately.
A more usual deployment might just need to investigate, and since the causes
could be many. Regardless, investigate the deployment as described in the
diagnosis, and contact the incident response team in your organization if
necessary.

[id="kubeapierrorbudgetburn"]
== KubeAPIErrorBudgetBurn

[discrete]
=== Meaning

https://github.com/openshift/cluster-kube-apiserver-operator/blob/622c08f101555be4584cb897f68f772777b32ada/bindata/v4.1.0/alerts/kube-apiserver-slos.yaml[These alerts] are triggered when the Kubernetes API
server is encountering:

* Many 5xx failed requests and/or
* Many slow requests

The urgency of these alerts is determined by the values of their `long` and
`short` labels:

* Critical
 ** `long`: 1h and `short`: 5m: less than ~2 days -- You should fix the problem
as soon as possible!
 ** `long`: 6h and `short`: 30m: less than ~5 days -- Track this down now but no
immediate fix required.
* Warning
 ** `long`: 1d and `short`: 2h: less than ~10 days -- This is problematic in the
long run. You should take a look in the next 24-48 hours.
 ** `long`: 3d and `short`: 6h: less than ~30 days -- (the entire window of the
error budget) at this rate. This means that at the end of the next 30 days there
won't be any error budget left at this rate. It's fine to leave this over the
weekend and have someone take a look in the coming days at working hours.

Information in this runbook is derived from the
https://github.com/prometheus-operator/kube-prometheus/wiki/KubeAPIErrorBudgetBurn[upstream runbook].

Note that in OCP 4.8, these alerts have been https://github.com/openshift/cluster-kube-apiserver-operator/pull/1126[rewritten], so they are
no longer the same as the upstream ones.

[discrete]
=== Impact

The overall availability of the cluster isn't guaranteed anymore. The API
server is returning too many errors and/or responses are taking too long for
guarantee reconciliation.

[discrete]
=== Diagnosis

The following https://github.com/openshift/cluster-kube-apiserver-operator/blob/c1c38912859e8b023a1da9168960e2c712068d5b/bindata/v4.1.0/alerts/kube-apiserver-slos.yaml#L234-L267[recording rules] can be used to determine the
main contributors of the alerts, after adjusting their range to match the `long`
and `short` labels of the active alerts:

[,sh]
----
# error
label_replace(
  sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1d]))
/ scalar(sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d])))
, "type", "error", "_none_", "")
or
# resource-scoped latency
label_replace(
  (
    sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="resource"}[1d]))
  -
    (sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="resource",le="0.1"}[1d])) or vector(0))
  ) / scalar(sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec"}[1d])))
, "type", "slow-resource", "_none_", "")
or
# namespace-scoped latency
label_replace(
  (
    sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="namespace"}[1d]))
  - sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="namespace",le="0.5"}[1d]))
  ) / scalar(sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec"}[1d])))
, "type", "slow-namespace", "_none_", "")
or
# cluster-scoped latency
label_replace(
  (
    sum(rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET",scope="cluster"}[1d]))
    - sum(rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[1d]))
  ) / scalar(sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d])))
, "type", "slow-cluster", "_none_", "")
----

In this example, the `slow-resource` error type appears to be the main
contributor to the alerts:

image::img/kubeapierrorbudgetburn-error-types.png[KubeAPIErrorBudgetBurn alert error types]

Use the `slow-resource` query from the above recording rules to identify the
resource kinds that contribute to the SLO violation:

[,sh]
----
sum by(resource) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="resource"}[1d]))
-
(sum by(resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="resource",le="0.1"}[1d])) or vector(0))
/ scalar(sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec"}[1d])))
----

In this example, requests to the `apirequestcounts` resource kind appear to be
the ones experiencing high latency:

image::img/kubeapierrorbudgetburn-slow-resource.png[KubeAPIErrorBudgetBurn slow resource]

If accessible, the following Grafana dashboards will can also provide further
insights into request duration and API server and etcd performance:

* API Request Duration by Verb
* etcd Request Duration - 99th Percentile
* etcd Object Count
* Request Duration by Read vs Write - 99th Percentile
* Long Running Requests by Resource

The following queries can be used individually to determine the resource kinds
that contribute to the SLO violation once the main contributor has been
identified.

`error`:

[,sh]
----
sum by(resource) (rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",code=~"5.."}[1d]))
/ scalar(sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d])) or vector(0))
----

`slow-resource`:

[,sh]
----
sum by(resource) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="resource"}[1d]))
-
(sum by(resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="resource",le="0.1"}[1d])) or vector(0))
/ scalar(sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec"}[1d])))
----

`slow-namespace`:

[,sh]
----
sum by(resource) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="namespace"}[1d]))
-
(sum by(resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec",scope="namespace",le="0.5"}[1d])) or vector(0))
/ scalar(sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET",subresource!~"proxy|log|exec"}[1d])))
----

`slow-cluster`:

[,sh]
----
sum by(resource) (rate(apiserver_request_duration_seconds_count{job="apiserver",verb=~"LIST|GET",scope="cluster"}[1d]))
-
(sum by(resource) (rate(apiserver_request_duration_seconds_bucket{job="apiserver",verb=~"LIST|GET",scope="cluster",le="5"}[1d])) or vector(0))
/ scalar(sum(rate(apiserver_request_total{job="apiserver",verb=~"LIST|GET"}[1d])))
----

[discrete]
=== Mitigation

[discrete]
==== Restart the kubelet (after upgrade)

If these alerts are triggered following a cluster upgrade, try restarting the
kubelets per description https://access.redhat.com/solutions/5420801[here].

[discrete]
==== Determine the source of the error or slow requests

There isn't a straightforward way to identify the root causes, but in the past
we were able to narrow down bugs by examining the failed resource counts in the
audit logs.

Gather the audit logs of the cluster:

[,sh]
----
oc adm must-gather -- /usr/bin/gather_audit_logs
----

Install the https://github.com/openshift/cluster-debug-tools[cluster-debug-tools] as a kubectl/oc plugin.

Use the `audit` subcommands to gather information on users that sends the
requests, the resource kinds, the request verbs etc.

E.g. to determine who generate the `apirequestcount` resource slow requests and
what these requests are doing:

[,sh]
----
oc dev_tool audit -f ${kube_apiserver_audit_log_dir} -otop --by=user resource="apirequestcounts"

oc dev_tool audit -f ${kube_apiserver_audit_log_dir} -otop --by=verb resource="apirequestcounts" --user=${top-user-from-last-command}
----

The `audit` subcommand also supports the `--failed-only` option which can be
used to return failed requests only:

[,sh]
----
# find the top-10 users with the highest failed requests count
oc dev_tool audit -f ${kube_apiserver_audit_log_dir} --by user --failed-only -otop

# find the top-10 failed API resource calls of a specific user
oc dev_tool audit -f ${kube_apiserver_audit_log_dir} --by resource --user=${service_account} --failed-only -otop

# find the top-10 failed API verbs of a specific user on a specific resource
oc dev_tool audit -f ${kube_apiserver_audit_log_dir} --by verb --user=${service_account} --resource=${resources} --failed-only -otop
----

When filing a new Bugzilla issue, be sure to attach this information and the
audit logs to it.

[id="extremelyhighindividualcontrolplanecpu"]
== ExtremelyHighIndividualControlPlaneCPU

[discrete]
=== Meaning

https://github.com/openshift/cluster-kube-apiserver-operator/blob/master/bindata/assets/alerts/cpu-utilization.yaml[This alert] is triggered when there
is a sustained high CPU utilization on a single control plane node.

The urgency of this alert is determined by how long the node is
sustaining high CPU usage:

* Critical
 ** when CPU usage on an individual control plane node is greater than
`90%` for more than `1h`.
* Warning
 ** when CPU usage on an individual control plane node is greater than
`90%` for more than `5m`.

https://github.com/openshift/cluster-kube-apiserver-operator/blob/master/bindata/assets/alerts/cpu-utilization.yaml[This alert] is triggered when CPU utilization
across all three control plane nodes is higher than two control plane nodes
can sustain; a single control plane node outage may cause
a cascading failure; increase available CPU.

The urgency of this alert is determined by how long CPU utilization across all
three control plane nodes is higher than two control plane nodes can sustain.

* Warning
 ** when CPU utilization across all three control plane nodes is higher than
two control plane nodes can sustain for more than `10m`.

[discrete]
=== Impact

Extreme CPU pressure can cause slow serialization and poor performance from
the `kube-apiserver` and `etcd`. When this happens, there is a risk of
clients seeing non-responsive API requests which are issued again
causing even more CPU pressure.

It can also cause failing liveness probes due to slow etcd responsiveness on
the backend. If one kube-apiserver fails under this condition, chances are
you will experience a cascade as the remaining kube-apiservers
are also under-provisioned.

To fix this, increase the CPU and memory on your control plane nodes.

[discrete]
=== Diagnosis

The following prometheus queries can be used to diagnose:

[,sh]
----
// Top 5 of containers with the most CPU utilization on a particular node
topk(5,
  sum by (namespace, pod, container) (
    irate (container_cpu_usage_seconds_total{node="NODE_NAME",container!="",pod!=""}[4m])
  )
)

// CPU utilization of containers on master nodes
sum by (node) (
  irate (container_cpu_usage_seconds_total{container!="",pod!=""}[4m])
    and on (node) cluster:master_nodes
)

// CPU utilization of the master nodes from the cgroups
sum by (node) (
  1 - irate(
    node_cpu_seconds_total{mode="idle"}[4m]
  )
  * on(namespace, pod) group_left(node) (
    node_namespace_pod:kube_pod_info:
  )
  and on (node) (
    cluster:master_nodes
  )
)

// for Windows
sum by (node) (
  1 - irate(
    windows_cpu_time_total{mode="idle", job="windows-exporter"}[4m]
  )
  and on (node) (
    cluster:master_nodes
  )
)
----

These are the conditions that could trigger the alert:

* there is a new workload that is generating more calls to the apiserver
and causing high CPU usage. In this case, increase the CPU and
memory on your control plane nodes.
* the alert is triggered based on the node metrics, so it could be that a
component on the node is causing the high CPU usage.
* apiserver/etcd is processing more requests due to client retries that is
being caused by an underlying condition.
* uneven distribution of requests to the apiserver instance(s) due to http2
(it multiplexes requests over a single TCP connection). The load balancers
are not at application layer, and so does not understand http2.

[discrete]
=== Mitigation

* if a workload is generating load to the apiserver that is causing high CPU
usage, then increase the CPU and memory on your control plane nodes.
* If the sustained high CPU usage is due to a cluster degradation:
 ** find out the root cause of the degradation, and then
determine the next steps accordingly.

If this needs to be reported, then capture the following dataset, and file
a new issue in BugZilla with links to the captured dataset:

* must-gather
* audit logs
* dump of prometheus data

How to gather the audit logs of the cluster:

[,sh]
----
oc adm must-gather -- /usr/bin/gather_audit_logs
----

How to take a dump of the cluster prometheus data:

[,sh]
----

#!/usr/bin/env bash

function queue() {
  local TARGET="${1}"
  shift
  local LIVE
  LIVE="$(jobs | wc -l)"
  while [[ "${LIVE}" -ge 45 ]]; do
    sleep 1
    LIVE="$(jobs | wc -l)"
  done
  echo "${@}"
  if [[ -n "${FILTER:-}" ]]; then
    "${@}" | "${FILTER}" >"${TARGET}" &
  else
    "${@}" >"${TARGET}" &
  fi
}

ARTIFACT_DIR=$PWD
mkdir -p $ARTIFACT_DIR/metrics
echo "Snapshotting prometheus (may take 15s) ..."
queue ${ARTIFACT_DIR}/metrics/prometheus.tar.gz oc --insecure-skip-tls-verify exec -n openshift-monitoring prometheus-k8s-0 -- tar cvzf - -C /prometheus .
FILTER=gzip queue ${ARTIFACT_DIR}/metrics/prometheus-target-metadata.json.gz oc --insecure-skip-tls-verify exec -n openshift-monitoring prometheus-k8s-0 -- /bin/bash -c "curl -G http://localhost:9090/api/v1/targets/metadata --data-urlencode 'match_target={instance!=\"\"}'"
----

[id="alertmanagerfailedreload"]
== AlertmanagerFailedReload

[discrete]
=== Meaning

The alert `AlertmanagerFailedReload` is triggered when the Alertmanager instance
for the cluster monitoring stack has consistently failed to reload its
configuration for a certain period.

[discrete]
=== Impact

Alerts for cluster components may not be delivered as expected.

[discrete]
=== Diagnosis

Check the logs for the `alertmanager-main` pods in the `openshift-monitoring`
namespace:

[source,terminal]
----
$ oc -n openshift-monitoring logs -l 'alertmanager=main'
----

[discrete]
=== Mitigation

The resolution depends on the particular issue reported in the logs.

[id="clusteroperatordegraded"]
== ClusterOperatorDegraded

[discrete]
=== Meaning

The alert `ClusterOperatorDegraded` is fired by
https://github.com/openshift/cluster-version-operator[cluster-version-operator](CVO)
when a `ClusterOperator` is in `degraded` state for a certain period. An
operator reports `Degraded` when its current state does not match its desired
state over a period resulting in a lower quality of service. The time may vary
by component, but a `Degraded` state represents the persistent observation of a
condition. A service state may be `Available` even when degraded. For example,
your service may desire three running pods, but one pod is in a crash-looping.
The service is `Available` but `Degraded` because it may have a lower quality of
service. A component may be `Progressing` but not `Degraded` because the
transition from one state to another does not persist over a long enough period
to report `Degraded`. A service should not report `Degraded` during a normal
upgrade. A service may report `Degraded` in response to a persistent
infrastructure failure that requires administrator intervention. For example,
when a control plane host is unhealthy and has to be replaced. An operator
should report `Degraded` if unexpected errors occur over a period, but the
expectation is that all unexpected errors are handled as operators mature.

[discrete]
=== Impact

An operator has encountered an error that is preventing it or its operand from
working properly. The operand may still be available, but its intent may not be
fulfilled. If this is true, it means that the operand is at risk of an outage or
improper configuration.

[discrete]
=== Diagnosis

The alert would convey exactly which operator the alert was fired for. The
operator name will be displayed under the `name` label. For example:

[,text]
----
 - alertname = ClusterOperatorDegraded
...
 - name = console
...
----

First, log in to the cluster. Multiple operators could be degraded at the same
time. Check the status of all operators to know whether there are more
`Degraded`:

[source,terminal]
----
$ oc get clusteroperator
----

Typically, the status will give some hint about the operator state.

[source,terminal]
----
$ oc get clusteroperator $CLUSTEROPERATOR -ojson | jq .status.conditions
----

Further on, if you would like to go through the associated resources for that
particular operator, you can use the command:

[source,terminal]
----
$ oc get clusteroperator $CLUSTEROPERATOR -ojson | jq .status.relatedObjects
----

Collect logs and artifacts for a given operator. As an example, you can collect
the logs of a specific operator and store them in a local directory named `out`
by using the following command:

[source,terminal]
----
$ oc adm inspect clusteroperator/$CLUSTEROPERATOR --dest-dir=out
----

[discrete]
=== Mitigation

The resolution steps would vary and depend on the particular `ClusterOperator`
that is in consideration. If there is an upgrade going on, then the `Degraded`
state may recover itself in some time. If a `ClusterOperator` is misconfigured,
then try to find the error in the collected logs and fix the configuration.

[id="clusteroperatordown"]
== ClusterOperatorDown

[discrete]
=== Meaning

The alert `ClusterOperatorDown` is fired by
https://github.com/openshift/cluster-version-operator[cluster-version-operator]
(CVO) when a `ClusterOperator` is not in the `Available` state for a certain
period. An operand that is functional in the cluster is `Available`.

[discrete]
=== Impact

There is an outage that needs to be checked at the earliest.

[discrete]
=== Diagnosis

The alert would convey exactly which operator the alert is for. The message will
contain the name. For example:

[,text]
----
 - alertname = ClusterOperatorDown
...
 - name = console
...
----

First, log in to the cluster. Multiple operators could be down at the same time.
Check the status of all operators to know whether more are not `Available`:

[source,terminal]
----
$ oc get clusteroperator
----

Typically, the status will give some hint about its state. Investigate further
for the operator that is not `Available` by using the following command:

[source,terminal]
----
$ oc get clusteroperator $CLUSTEROPERATOR -ojson | jq .status.conditions
----

Further on, if you would like to go through the associated resources for that
particular operator, you can use the command:

[source,terminal]
----
$ oc get clusteroperator $CLUSTEROPERATOR -ojson | jq .status.relatedObjects
----

Collect logs and artifacts for a given operator. As an example, you can collect
the logs of a specific operator and store them in a local directory named `out`
by using the following command:

[source,terminal]
----
$ oc adm inspect clusteroperator/$CLUSTEROPERATOR --dest-dir=out
----

[discrete]
=== Mitigation

The resolution steps would vary and depend on the particular operator that is in
consideration. If there is an upgrade going on, then this issue may resolve
itself after some time. Otherwise, try to find an error in the logs.

[id="kubeapidown"]
== KubeAPIDown

[discrete]
=== Meaning

The `KubeAPIDown` alert is triggered when all Kubernetes API servers have not
been reachable by the monitoring system for more than 15 minutes.

[discrete]
=== Impact

This is a critical alert. The Kubernetes API is not responding. The
cluster may partially or fully non-functional.

[discrete]
=== Diagnosis

Check the status of the API server targets in the Prometheus UI.

Then, confirm whether the API is also unresponsive for you:

[source,terminal]
----
$ oc cluster-info
----

If you can still reach the API server, there may be a network issue between the
Prometheus instances and the API server pods. Check the status of the API server
pods:

[source,terminal]
----
$ oc -n openshift-kube-apiserver get pods
$ oc -n openshift-kube-apiserver logs -l 'app=openshift-kube-apiserver'
----

[discrete]
=== Mitigation

If you can still reach the API server intermittently, you may be able treat this
like any other failing deployment. If not, it's possible you may have to refer
to the disaster recovery documentation for your version of OpenShift.

[id="kubedeploymentreplicasmismatch"]
== KubeDeploymentReplicasMismatch

[discrete]
=== Meaning

This alert is fired when a discrepancy between the desired number of replicas to
the actual number of running instances for deployment was observed for a certain
period.

[discrete]
=== Impact

The impact very much differs depending on the discrepancy.

[discrete]
=== Diagnosis

The alert should note where the discrepancy occurred under the `deployment`
label:

[source,terminal]
----
 - alertname = KubeDeploymentReplicasMismatch
...
 - deployment = elasticsearch-cdm-u1gqqbu6-2
...
 - namespace = openshift-logging
...
----

Start by checking the status of the deployment:

[source,terminal]
----
$ oc get deploy -n $NAMESPACE $DEPLOYMENT
----

Review the current deployment using the details available in the alert. Review
the following in the target namespace to ascertain the reason behind this. The
events:

[source,terminal]
----
$ oc get events -n $NAMESPACE
----

Further, check the states of the Pods that the deployment manages:

[source,terminal]
----
$ oc get pods -n $NAMESPACE --selector=app=$DEPLOYMENT
----

Possibilities include (but are not limited to) a pod stuck in
`ContainerCreating` or `CrashLoopBackoff`. The events may list this case
information about possible failed actions of a pod. Application and startup
failures should be visible with:

[source,terminal]
----
$ oc describe pod $POD
----

If Pods are stuck in `Pending`, it means that insufficient resources prevent the
pod from being scheduled. Check the health of the nodes.

[source,terminal]
----
$ oc get nodes
----

It is further possible that the CPU and Memory of the host are exhausted.

[source,terminal]
----
$ oc adm top nodes
----

[discrete]
=== Mitigation

Resolve the problems discovered during the diagnosis according to the
documentation. It is safe to delete the pods since they are managed by the
deployment. However, it may also be required to add more nodes in case of
insufficient resources.

[id="kubejobfailed"]
== KubeJobFailed

[discrete]
=== Meaning

https://github.com/openshift/cluster-monitoring-operator/blob/aefc8fc5fc61c943dc1ca24b8c151940ae5f8f1c/assets/control-plane/prometheus-rule.yaml#L186-L195[This alert] is triggered for the case that the number of job
execution attempts exceeds the `backoffLimit`. A job can therefore create one or
many pods for its tasks.

[discrete]
=== Impact

A task has not finished correctly. Depending on the task, this has a different
impact.

[discrete]
=== Diagnosis

The alert should contain the job name and the namespace where that job failed.
Follow the particular mitigation steps according to that. For example:

[,text]
----
 - alertname = KubeJobFailed
...
 - job_name = elasticsearch-delete-app-1600903800
 - namespace = openshift-logging
...
 - message = Job openshift-logging/elasticsearch-delete-app-1600855200 failed to complete.
----

[discrete]
=== Mitigation

Find the pods that belong to that job:

[source,terminal]
----
$ oc get pod -n $NAMESPACE -l job-name=$JOBNAME
----

If you see an `Error` pod with a subsequent `Completed` pod of
the same base name, the error was transient, and the `Error` pod can safely be deleted.

Have a look at the jobs themselves:

[source,terminal]
----
$ oc get jobs -n $NAMESPACE
----

If there is a healthy job for every failed one, it is safe to delete the failed
jobs. The alert should resolve itself after a few minutes.

[id="kubenodenotready"]
== KubeNodeNotReady

[discrete]
=== Meaning

https://github.com/openshift/cluster-monitoring-operator/blob/aefc8fc5fc61c943dc1ca24b8c151940ae5f8f1c/assets/control-plane/prometheus-rule.yaml#L482-L490[This alert] is fired when a Kubernetes node is not in `Ready`
state for a certain period. In this case, the node is not able to host any new
pods as described https://kubernetes.io/docs/concepts/architecture/nodes/#condition[here].

[discrete]
=== Impact

The performance of the cluster deployments is affected, depending on the overall
workload and the type of the node.

[discrete]
=== Diagnosis

The notification details should list the node that's not ready. For Example:

[,txt]
----
 - alertname = KubeNodeNotReady
...
 - node = node1.example.com
...
----

Login to the cluster. Check the status of that node:

[source,terminal]
----
$ oc get node $NODE -o yaml
----

The output should describe why the node isn't ready (e.g.: timeouts reaching the
API or kubelet) Check the machine for the node:

[source,terminal]
----
$ oc get -n openshift-machine-api machine $NODE -o yaml
----

and the events for the machine API:

[source,terminal]
----
$ oc get -n openshift-machine-api events
----

If the machine API is not able to replace the node, the machine status and
events should detail why.

[discrete]
=== Mitigation

Once, the problem was resolved that prevented the machine API from replacing the
node, the instance should be terminated and replaced by the machine API.
However, this is only the case `MachineHealthChecks` are enabled for the nodes
otherwise a manual restart is required.

[id="kubepersistentvolumefillingup"]
== KubePersistentVolumeFillingUp

[discrete]
=== Meaning

This alert fires when a persistent volume in one of the system namespaces,
i.e. a namespace beginning with `openshift-`, `kube-`, or the `default`
namespace, has less than 3% of its total space left.

[discrete]
=== Impact

A full persistent volume used by a system component is likely to prevent the
component from functioning normally, and may lead to a partial or full cluster
outage.

[discrete]
=== Diagnosis

The alert labels should include the name of the PersistentVolumeClaim associated
with the volume that is low on storage, as well as the namespace that claim is
in.  You can use these to graph the available storage in the OpenShift web
console under Observer \-> Metrics.  The following is an example query for a
volume claim associated with a Prometheus instance in the `openshift-monitoring`
namespace:

[,text]
----
kubelet_volume_stats_available_bytes{
  namespace="openshift-monitoring",
  persistentvolumeclaim="prometheus-k8s-db-prometheus-k8s-0"
}
----

You can inspect the contents of the volume manually to determine what is using
the storage:

[source,terminal]
----
$ PVC_NAME='<persistentvolumeclaim label from alert>'
$ NAMESPACE='<namespace label from alert>'

$ oc -n $NAMESPACE describe pvc $PVC_NAME
$ POD_NAME='<"Used By:" field from the above output>'

$ oc -n $NAMESPACE rsh $POD_NAME
$ df -h
----

[discrete]
=== Mitigation

The mitigation largely depends on what is using the storage.  You may be able to
simply allocate more storage to the affected volume, or adjust the configuration
for the component using the volume to use less space -- for instance by lowering
a logging level or similar.

[id="kubepodnotready"]
== KubePodNotReady

[discrete]
=== Meaning

https://github.com/openshift/cluster-monitoring-operator/blob/aefc8fc5fc61c943dc1ca24b8c151940ae5f8f1c/assets/control-plane/prometheus-rule.yaml#L26-L41[This alert] is fired when some pods have not been in the
`Ready` state for a certain period. This can have different reasons as described
https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/[here]: When the pod is `Running` but not `Ready`, the `Readiness`
probe is failing. An application-specific error may prevent the pod from being
attached to a service. When the pod remains in `Pending` it can not be deployed
to particular namespaces and nodes.

[discrete]
=== Impact

This pod is not functional and doesn't receive any traffic. Depending on how
many functional replicas are still available, the impact differs.

[discrete]
=== Diagnosis

The notification details should list the pod that's not ready (and the pod's
namespace). E.g.:

[,text]
----
 - alertname = KubePodNotReady
...
 - namespace = openshift-logging
 - pod = elasticsearch-cdm-u1gqqbu6-2-868ddd4b45-w224d
...
----

Start by checking the status of the pod:

[source,terminal]
----
$ oc get pod -n $NAMESPACE $POD
----

When the pod state is in `Running`, you can check its logs:

[source,terminal]
----
$ oc logs -n $NAMESPACE $POD
----

Be aware there may be multiple containers in the pod. A check of all their logs
may be required. If the pod isn't running (for instance, if it's stuck in
`ContainerCreating`), then try to find out why.

[discrete]
=== Mitigation

Try to find the issue in the logs before deleting it.

[id="kubeletdown"]
== KubeletDown

[discrete]
=== Meaning

This alert is triggered when the monitoring system has not been able to reach
any of the cluster's Kubelets for more than 15 minutes.

[discrete]
=== Impact

This alert represents a critical threat to the cluster's stability. Excluding
the possibility of a network issue preventing the monitoring system from
scraping Kubelet metrics, multiple nodes in the cluster are likely unable to
respond to configuration changes for pods and other resources, and some
debugging tools are likely not functional, e.g. `oc exec` and `oc logs`.

[discrete]
=== Diagnosis

Check the status of nodes and for recent events on `Node` objects, or for recent
events in general:

[source,terminal]
----
$ oc get nodes
$ oc describe node $NODE_NAME
$ oc get events --field-selector 'involvedObject.kind=Node'
$ oc get events
----

If you have SSH access to the nodes, access the logs for the Kubelet directly:

[source,terminal]
----
$ journalctl -b -f -u kubelet.service
----

[discrete]
=== Mitigation

The mitigation depends on what is causing the Kubelets to become
unresponsive. Check for wide-spread networking issues, or node level
configuration issues.

[id="nodefiledescriptorlimit"]
== NodeFileDescriptorLimit

[discrete]
=== Meaning

This alert is triggered when a node's kernel is found to be running out of
available file descriptors -- a `warning` level alert at greater than 70% usage
and a `critical` level alert at greater than 90% usage.

[discrete]
=== Impact

Applications on the node may no longer be able to open and operate on
files. This is likely to have severe consequences for anything scheduled on this
node.

[discrete]
=== Diagnosis

You can open a shell on the node and use the standard Linux utilities to
diagnose the issue:

[source,terminal]
----
$ NODE_NAME='<value of instance label from alert>'

$ oc debug "node/$NODE_NAME"
# sysctl -a | grep 'fs.file-'
fs.file-max = 1597016
fs.file-nr = 7104       0       1597016
# lsof -n
----

[discrete]
=== Mitigation

Reduce the number of files opened simultaneously by either adjusting application
configuration or by moving some applications to other nodes.

[id="nodefilesystemalmostoutoffiles"]
== NodeFilesystemAlmostOutOfFiles

[discrete]
=== Meaning

This alert is similar to the https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md[NodeFilesystemSpaceFillingUp] alert, but rather
than being based on a prediction that a filesystem will run out of inodes in a
certain amount of time, it uses simple static thresholds. The alert will fire as
at a `warning` level at 5% of available inodes left, and at a `critical` level
with 3% of available inodes left.

[discrete]
=== Impact

A node's filesystem becoming full can have a far reaching impact, as it may
cause any or all of the applications scheduled to that node to experience
anything from performance degradation to full inoperability. Depending on the
node and filesystem involved, this could pose a critical threat to the stability
of the cluster.

[discrete]
=== Diagnosis

Refer to the https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md[NodeFilesystemFilesFillingUp] runbook.

[discrete]
=== Mitigation

Refer to the https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemFilesFillingUp.md[NodeFilesystemFilesFillingUp] runbook.

[id="nodefilesystemalmostoutofspace"]
== NodeFilesystemAlmostOutOfSpace

[discrete]
=== Meaning

This alert is similar to the https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md[NodeFilesystemSpaceFillingUp] alert, but rather
than being based on a prediction that a filesystem will become full in a certain
amount of time, it uses simple static thresholds. The alert will fire as at a
`warning` level at 5% space left, and at a `critical` level with 3% space left.

[discrete]
=== Impact

A node's filesystem becoming full can have a far reaching impact, as it may
cause any or all of the applications scheduled to that node to experience
anything from performance degradation to full inoperability. Depending on the
node and filesystem involved, this could pose a critical threat to the stability
of the cluster.

[discrete]
=== Diagnosis

Refer to the https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md[NodeFilesystemSpaceFillingUp] runbook.

[discrete]
=== Mitigation

Refer to the https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md[NodeFilesystemSpaceFillingUp] runbook.

[id="nodefilesystemfilesfillingup"]
== NodeFilesystemFilesFillingUp

[discrete]
=== Meaning

This alert is similar to the https://github.com/openshift/runbooks/blob/master/alerts/cluster-monitoring-operator/NodeFilesystemSpaceFillingUp.md[NodeFilesystemSpaceFillingUp] alert, but
predicts the filesystem will run out of inodes rather than bytes of storage
space. The alert fires at a `critical` level when the filesystem is predicted to
run out of available inodes within four hours.

[discrete]
=== Impact

A node's filesystem becoming full can have a far reaching impact, as it may
cause any or all of the applications scheduled to that node to experience
anything from performance degradation to full inoperability. Depending on the
node and filesystem involved, this could pose a critical threat to the stability
of the cluster.

[discrete]
=== Diagnosis

Note the `instance` and `mountpoint` labels from the alert. You can graph the
usage history of this filesystem with the following query in the OpenShift web
console:

[,text]
----
node_filesystem_files_free{
  instance="<value of instance label from alert>",
  mountpoint="<value of mountpoint label from alert>"
}
----

You can also open a debug session on the node and use the standard Linux
utilities to locate the source of the usage:

[source,terminal]
----
$ MOUNT_POINT='<value of mountpoint label from alert>'
$ NODE_NAME='<value of instance label from alert>'

$ oc debug "node/$NODE_NAME"
$ df -hi "/host/$MOUNT_POINT"
----

Note that in many cases a filesystem running out of inodes will still have
available storage. Running out of inodes is often caused by many many small
files being created by an application.

[discrete]
=== Mitigation

The number of inodes allocated to a filesystem is usually based on the storage
size. You may be able to solve the problem, or buy time, by increasing size of
the storage volume. Otherwise, determine the application that is creating large
numbers of files and adjust its configuration or provide it dedicated storage.

[id="nodefilesystemspacefillingup"]
== NodeFilesystemSpaceFillingUp

[discrete]
=== Meaning

This alert is based on an extrapolation of the space used in a file system. It
fires if both the current usage is above a certain threshold _and_ the
extrapolation predicts to run out of space in a certain time. This is a
warning-level alert if that time is less than 24h. It's a critical alert if that
time is less than 4h.

[discrete]
=== Impact

A filesystem running full is very bad for any process in need to write to the
filesystem. But even before a filesystem runs full, performance is usually
degrading.

[discrete]
=== Diagnosis

Study the recent trends of filesystem usage on a dashboard. Sometimes a periodic
pattern of writing and cleaning up can trick the linear prediction into a false
alert. Use the usual OS tools to investigate what directories are the worst
and/or recent offenders. Is this some irregular condition, e.g. a process fails
to clean up behind itself or is this organic growth? If monitoring is enabled,
the following metric can be watched in PromQL.

[source,terminal]
----
node_filesystem_free_bytes
----

Check the alert's `mountpoint` label.

[discrete]
=== Mitigation

For the case that the `mountpoint` label is `/`, `/sysroot` or `/var`; then
removing unused images solves that issue:

Debug the node by accessing the node filesystem:

[source,terminal]
----
$ NODE_NAME=<instance label from alert>
$ oc -n default debug node/$NODE_NAME
$ chroot /host
----

Remove dangling images:

[source,terminal]
----
$ podman images -q -f dangling=true | xargs --no-run-if-empty podman rmi
----

Remove unused images:

[source,terminal]
----
$ podman images | grep -v -e registry.redhat.io -e "quay.io/openshift" -e registry.access.redhat.com -e docker-registry.usersys.redhat.com -e docker-registry.ops.rhcloud.com -e rhmap | xargs --no-run-if-empty podman rmi 2>/dev/null
----

Exit debug:

[source,terminal]
----
$ exit
$ exit
----

[id="noderaiddegraded"]
== NodeRAIDDegraded

[discrete]
=== Meaning

This alert is triggered when a node has a storage configuration with RAID array,
and the array is reporting as being in a degraded state due to one or more disk
failures.

[discrete]
=== Impact

The affected node could go offline at any moment if the RAID array fully fails
due to further issues with disks.

[discrete]
=== Diagnosis

You can open a shell on the node and use the standard Linux utilities to
diagnose the issue, but you may need to install additional software in the debug
container:

[source,terminal]
----
$ NODE_NAME='<value of instance label from alert>'

$ oc debug "node/$NODE_NAME"
$ cat /proc/mdstat
----

[discrete]
=== Mitigation

See the Red Hat Enterprise Linux https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/managing_storage_devices/managing-raid_managing-storage-devices[documentation].

[id="prometheustargetsyncfailure"]
== PrometheusTargetSyncFailure

[discrete]
=== Meaning

This alert is triggered when at least one of the Prometheus instances has
consistently failed to sync its configuration.

[discrete]
=== Impact

Metrics and alerts may be missing or inaccurate.

[discrete]
=== Diagnosis

Determine whether the alert is for the cluster or user workload Prometheus by
inspecting the alert's `namespace` label. The namespace for the cluster
Prometheus is `openshift-monitoring`, while the namespace for the user workload
monitoring instance is `openshift-user-workload-monitoring`.

Check the logs for the appropriate Prometheus instance:

[source,terminal]
----
$ NAMESPACE='<value of namespace label from alert>'

$ oc -n $NAMESPACE logs -l 'app=prometheus'
level=error ... msg="Creating target failed" ...
----

[discrete]
=== Mitigation

If the logs indicate a syntax or other configuration error, correct the
corresponding `ServiceMonitor`, `PodMonitor`, or other configuration
resource. In most all cases, the operator should prevent this from happening.

[id="thanosrulequeueisdroppingalerts"]
== ThanosRuleQueueIsDroppingAlerts

[discrete]
=== Meaning

The https://thanos.io/v0.22/components/rule.md[Thanos Ruler] is deployed when user workload monitoring is enabled. It
allows alerting rules deployed as part of user workload monitoring to query both
the the Prometheus instance responsible for cluster components as well the user
workload Prometheus instance. This alert is triggered when the Thanos Ruler
queue is found to be dropping alerting events.

[discrete]
=== Impact

Alerts for user workloads may not be delivered.

[discrete]
=== Diagnosis

Check the logs for the Thanos Rules pods:

[source,terminal]
----
$ oc -n openshift-user-workload-monitoring logs -l 'thanos-ruler=user-workload'
...
level=warn ... msg="Alert notification queue full, dropping alerts" numDropped=100
level=warn ... msg="Alert batch larger than queue capacity, dropping alerts" numDropped=100
----

If this alert has triggered, you likely have an extremely large number of alerts
flowing from the user workload monitoring stack. Log into the OpenShift web
console and check the active alerts.

[discrete]
=== Mitigation

The default queue capacity for Thanos Ruler is quite high at 10,000 items,
meaning the most likely scenario is a misconfiguration causing the user workload
monitoring stack to overload Thanos Ruler with duplicate or otherwise erroneous
alerts. Check the active alerts in the OpenShift web console, and correct any
misconfigurations or consider grouping alerts.

